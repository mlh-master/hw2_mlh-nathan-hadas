{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answers for Theory Questions*, edited by **Hadas Ben-Atya and Nathan Berdugo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1**\n",
    "\n",
    "The evaluation metric which is more important to us is the model performance. This metric provides us detailed and objective information about our model, which will be useful in finding flaws in our model and therefore to improve it. The mentioned provided parameters would be Sensitivity and Specificity, in addition to F1, AUC, PPV, NPV and much more. All of this parameters refer to the model performance according to the data on which we performed the training, regardless of the drawbacks of the data itself.\n",
    "All of the above isn't necessarily correct when using the Accuracy as an evaluation metric; A simple example will demonstrate it. Assume you are trying to predict the prevalence of an extremely rare disease in a randomly chosen population. One could use a naive estimator that predicts all of the population to be healthy. The accuracy model will indicate excellent results (because as mentioned, the disease is extreamly rare). This evaluation will not provide an objective, data-independent result. therefore, it's much more efficient to choose a model performance as our evalutaion metric, even when dealing with a reasonable prevalence in the population such in our T1D classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2**\n",
    "\n",
    "In the process of choosing between the two classifiers, one should rely on numerous criteria which provide helpful guidlines.\n",
    "\n",
    "The first is Accuracy. The more we use relevant features, the more accurate our model will be. We should use our common sense in order to confirm the need of the feature, in addition to visualization of the features' correlation. We will always try to avoid ignoring important data that were already collected, and which will serve as useful information for our prediction model.\n",
    "In the first classifier we use BP and BMI, as mentioned in the question, but we dismiss potentially-relevant features such as the Age of the patient and his level of physical activity, which might imply on his probability to get an income (a heart-attack). In the second model we use all the mentioned features, which some of them may turn out to be useless, and should have been dismissed. This leads us to the second critaria - Complexity.\n",
    "\n",
    "If we choose to include useless information, we force ourselves to use greater computational resources that might be expensive or even inaccessible for us. When choosing to reduce complexity by ignoring features (as with the first classifier), we can make a much more time-efficient and cost-saving model, that will use only relevant data and dismiss the rest of it. Moreover, the higher the number of features we use, the larger the number of samples will be needed in order to reach statistical significance. Thus by choosing all-features classifier, an in-depth research will be required to obtain sufficient data, which may lead again to an increase in financial expenses.\n",
    "\n",
    "The third criteria is Generalization. It refers to how well the concepts learned by a machine learning model apply to specific examples not seen by the model when it was learning. By including useless features, we let irrelevant data influence our model \n",
    "without improving the predictive capabilities. Such noise can adversely affect the model and impair its generalization capabilities when using testing data, because of overfitting to the training data and higher variance. The all-features classifier might lead to this result, while the first 2-features-only classifier might result in underfitting and high bias - the opposite scenario of the above - which lead to a model that can neither model the training data nor generalize to new (testing) data.\n",
    "\n",
    "The over/underfitting both lead to poor performance and could be avoided by correctly choosing the features to include. Generalization is what we get when we find the sweet spot between these two edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3**\n",
    "\n",
    "Considering the case described, in which we are required to distinguish between types of pancreatic biopsies showing T1D signs and those that do not, based on the measurements specified in the question, we see no reason to assume that the given classification problem is linear separable, given the amount and type of parameters that leads to our assumption that there is linear dependence between them (e.g. A common sense will lead to the idea that the bigger the size of the biopsy, the larger will be the cell-count).\n",
    "\n",
    "Thus, it can be assumed that the best prediction model in this case would be a non-linear one - for example Support-Vector-Machine (SVM) with a non-linear kernel.\n",
    "\n",
    "The other two models suggested, LR and linear SVM, assume linear separations of the features and therefore we assume that they are less suitable in the case described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4**\n",
    "\n",
    "While Logistic regression (LR) and Support-Vector-Machine (SVM) are closely linked, there a few major differences between them that can be summarized in several aspects:\n",
    "\n",
    "Logistic regression (LR) focuses on maximizing the probability of the data, while it rarely include maximal nor minimal values (1/0 respectively) even when the data is indeed located near the edges. This may impair the accuracy of the model. LR returns calibrated probabilities that help us quantify the confidence\n",
    "\n",
    "SVM (in general, not necessarily SVM with linear kernel), tries to maximize the distance of the two closest samples by finding the best separating hyperplane that does that. When the hyperplane can be easily achieved by drawing a straight line that correctly separates our data, it is called linear SVM. Otherwise, we use non-linear SVM which transforms our data (by using kernel functions) into a higher dimensional space, resulting with a hyperplane that can now separate the data.\n",
    "\n",
    "While SVM is based mostly on geometrical properties of the data, LR relies much more on statistical preoperties.\n",
    "\n",
    "In addition, as explained above, while SVM maximize the distance between the hyperplane and the support vectors in each side of it, resulting in a reduced risk of misclassification of the data, LR uses different considerations (mostly statistic-based) in order to find the best separation. [1] [2]\n",
    "\n",
    "Regarding difference in the concept of their hyperparameters tuning:\n",
    "\n",
    "When discussing about *LR and SVM hyperparameters*, we mostly refer to C, or its inverse - Lambda.\n",
    "\n",
    "In SVM, This hyperparameter controls the trade-off between {increasing the distance between the hyperplane and the support vectors}, and {decreasing the number of samples which are misclassified by this hyperplane}. C does it by penalizing for each misclassified point. The higher C is, the higher the pealty for each misclassified sample is, in the expense of decreasing the distance between the hyperplane and the support vectors.\n",
    "\n",
    "In LR, C controls the trade-off between {allowing the model to increase its complexity by using all the features it can get}, and {keeping the model as simple as possible}. The higher C is, the higher the complexity of our model , and the probability of reaching overfitting increases. In this case (of LR), C works by assigning weights for each parameter included in the model. Thus,  according to the example above - a higher value of C will lead to large weights assigning.\n",
    "\n",
    "In contrary to SVM - where C serves as a loss-function effector - in LR, C has a weightening effect.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "[1] SVM vs LR, Kevin Swersky, University of Toronto, http://www.cs.toronto.edu/~kswersky/wp-content/uploads/svm_vs_lr.pdf\n",
    "[2] Logistic Regression Vs Support Vector Machines (SVM), Patricia Bassey, Axum Labs, https://medium.com/axum-labs/logistic-regression-vs-support-vector-machines-svm-c335610a3d16#:~:text=SVM%20tries%20to%20finds%20the,are%20near%20the%20optimal%20point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
